{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a0f402a6-c9ba-4121-87d0-3481b4518ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully saved to /media/lab/My Passport/hail/gfs/u_wind_200hPa.nc\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import os\n",
    "\n",
    "# Folder containing your GRIB files\n",
    "input_folder = '/media/lab/My Passport/hail/gfs/20190315/'\n",
    "# List of GRIB files to process\n",
    "grib_files = [\n",
    "    f\"gfs.0p25.2020032300.f{str(i).zfill(3)}.grib2\" for i in range(0, 25, 3)\n",
    "]\n",
    "# Path for the output NetCDF file\n",
    "output_file = '/media/lab/My Passport/hail/gfs/temperature'\n",
    "\n",
    "# Open multiple GRIB files and extract data for the u-component of wind at 200 hPa level\n",
    "datasets = []\n",
    "for grib_file in grib_files:\n",
    "    file_path = os.path.join(input_folder, grib_file)\n",
    "    ds = xr.open_dataset(file_path, engine='cfgrib', \n",
    "                         backend_kwargs={'filter_by_keys': {'typeOfLevel': 'isobaricInhPa', \n",
    "                                                            'shortName': 't', \n",
    "                                                            'level': 200}})\n",
    "    datasets.append(ds)\n",
    "\n",
    "# Concatenate all the datasets along the time dimension and save to NetCDF\n",
    "combined_ds = xr.concat(datasets, dim='time')\n",
    "combined_ds.to_netcdf(output_file)\n",
    "print(f\"Data successfully saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "db104d03-e717-49c4-9ef2-d9e4bbf15f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully saved to /media/lab/My Passport/hail/gfs/temperature.nc\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import os\n",
    "\n",
    "# Folder containing your GRIB files\n",
    "input_folder = '/media/lab/My Passport/hail/gfs/20190315/'\n",
    "# List of GRIB files to process\n",
    "grib_files = [\n",
    "    f\"gfs.0p25.2019031500.f{str(i).zfill(3)}.grib2\" for i in range(0, 25, 3)\n",
    "]\n",
    "# Path for the output NetCDF file\n",
    "output_file = '/media/lab/My Passport/hail/gfs/temperature.nc'\n",
    "\n",
    "# Open multiple GRIB files and extract data for temperature at all isobaricInhPa levels\n",
    "datasets = []\n",
    "for grib_file in grib_files:\n",
    "    file_path = os.path.join(input_folder, grib_file)\n",
    "    try:\n",
    "        # Open dataset for Temperature at all isobaricInhPa levels\n",
    "        ds = xr.open_dataset(file_path, engine='cfgrib', \n",
    "                             backend_kwargs={'filter_by_keys': {'typeOfLevel': 'isobaricInhPa', \n",
    "                                                                'shortName': 't'}})\n",
    "        datasets.append(ds)\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping file {grib_file} due to error: {e}\")\n",
    "\n",
    "# Concatenate all the datasets along the time dimension if any datasets were successfully loaded\n",
    "if datasets:\n",
    "    combined_ds = xr.concat(datasets, dim='time', coords='minimal', compat='override')\n",
    "    combined_ds.to_netcdf(output_file)\n",
    "    print(f\"Data successfully saved to {output_file}\")\n",
    "else:\n",
    "    print(\"No data extracted. Please check the input files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "afa237ed-c0e1-487c-95db-ef10be5605c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully saved to /media/lab/My Passport/hail/gfs/geopotential_height_all_levels.nc\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import os\n",
    "\n",
    "# Folder containing your GRIB files\n",
    "input_folder = '/media/lab/My Passport/hail/gfs/20190315/'\n",
    "# List of GRIB files to process\n",
    "grib_files = [\n",
    "    f\"gfs.0p25.2019031500.f{str(i).zfill(3)}.grib2\" for i in range(0, 25, 3)\n",
    "]\n",
    "# Path for the output NetCDF file\n",
    "output_file = '/media/lab/My Passport/hail/gfs/geopotential_height_all_levels.nc'\n",
    "\n",
    "# List of pressure levels in hPa to extract for Geopotential Height\n",
    "pressure_levels = [\n",
    "    1, 2, 3, 5, 7, 10, 20, 30, 50, 70, 100, 150, 200, 250, 300, 350, 400,\n",
    "    450, 500, 550, 600, 650, 700, 750, 800, 850, 900, 925, 950, 975, 1000\n",
    "]\n",
    "\n",
    "# List to store datasets for each file and pressure level\n",
    "datasets = []\n",
    "\n",
    "for grib_file in grib_files:\n",
    "    file_path = os.path.join(input_folder, grib_file)\n",
    "    try:\n",
    "        # Open dataset for Geopotential Height at all specified isobaric levels\n",
    "        ds = xr.open_dataset(file_path, engine='cfgrib', \n",
    "                             backend_kwargs={'filter_by_keys': {\n",
    "                                 'typeOfLevel': 'isobaricInhPa', \n",
    "                                 'shortName': 'gh'\n",
    "                             }})\n",
    "        \n",
    "        # Select only the specified pressure levels\n",
    "        ds = ds.sel(isobaricInhPa=pressure_levels)\n",
    "        \n",
    "        # Append to the list if successful\n",
    "        datasets.append(ds)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Skipping file {grib_file} due to error: {e}\")\n",
    "\n",
    "# Concatenate all the datasets along the time dimension if any datasets were successfully loaded\n",
    "if datasets:\n",
    "    combined_ds = xr.concat(datasets, dim='time', coords='minimal', compat='override')\n",
    "    combined_ds.to_netcdf(output_file)\n",
    "    print(f\"Data successfully saved to {output_file}\")\n",
    "else:\n",
    "    print(\"No data extracted. Please check the input files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f89efdd8-6f17-4d6a-bb07-7fbc796e525f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully saved to /media/lab/My Passport/hail/gfs/20200323/temp.nc\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import os\n",
    "\n",
    "# Folder containing your GRIB files\n",
    "input_folder = '/media/lab/My Passport/hail/gfs/20200323/'\n",
    "# List of GRIB files to process\n",
    "grib_files = [\n",
    "    f\"gfs.0p25.2020032300.f{str(i).zfill(3)}.grib2\" for i in range(0, 25, 3)\n",
    "]\n",
    "# Path for the output NetCDF file\n",
    "output_file = '/media/lab/My Passport/hail/gfs/20200323/temp.nc'\n",
    "\n",
    "# Open multiple GRIB files and extract data for temperature at all isobaricInhPa levels\n",
    "datasets = []\n",
    "for grib_file in grib_files:\n",
    "    file_path = os.path.join(input_folder, grib_file)\n",
    "    try:\n",
    "        # Open dataset for Temperature at all isobaricInhPa levels\n",
    "        ds = xr.open_dataset(file_path, engine='cfgrib', \n",
    "                             backend_kwargs={'filter_by_keys': {'typeOfLevel': 'isobaricInhPa', \n",
    "                                                                'shortName': 'cape'}})\n",
    "        datasets.append(ds)\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping file {grib_file} due to error: {e}\")\n",
    "\n",
    "# Concatenate all the datasets along the time dimension if any datasets were successfully loaded\n",
    "if datasets:\n",
    "    combined_ds = xr.concat(datasets, dim='time', coords='minimal', compat='override')\n",
    "    combined_ds.to_netcdf(output_file)\n",
    "    print(f\"Data successfully saved to {output_file}\")\n",
    "else:\n",
    "    print(\"No data extracted. Please check the input files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d81a40c-2b31-427b-85c7-a95755090bf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bc6e39e5-8f92-49aa-864c-6c5375632465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully saved to /media/lab/My Passport/hail/gfs/20190315/u_wind_all_levels1.nc\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import os\n",
    "\n",
    "# Folder containing your GRIB files\n",
    "input_folder = '/media/lab/My Passport/hail/gfs/20190315/'\n",
    "# List of GRIB files to process\n",
    "grib_files = [\n",
    "    f\"gfs.0p25.2019031500.f{str(i).zfill(3)}.grib2\" for i in range(0, 25, 3)\n",
    "]\n",
    "# Path for the output NetCDF file\n",
    "output_file = '/media/lab/My Passport/hail/gfs/20190315/u_wind_all_levels1.nc'\n",
    "\n",
    "# Open multiple GRIB files and extract data for the u-component of wind at all isobaricInhPa levels\n",
    "datasets = []\n",
    "for grib_file in grib_files:\n",
    "    file_path = os.path.join(input_folder, grib_file)\n",
    "    ds = xr.open_dataset(file_path, engine='cfgrib', \n",
    "                         backend_kwargs={'filter_by_keys': {'typeOfLevel': 'isobaricInhPa', \n",
    "                                                            'shortName': 'u'}})\n",
    "    datasets.append(ds)\n",
    "\n",
    "# Concatenate all the datasets along the time dimension and save to NetCDF\n",
    "combined_ds = xr.concat(datasets, dim='time')\n",
    "combined_ds.to_netcdf(output_file)\n",
    "print(f\"Data successfully saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f3b96815-6c9c-40ae-9db1-0e6190abe992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully saved to /media/lab/My Passport/hail/gfs/20190315/u_wind_all_levels1.nc\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import os\n",
    "\n",
    "# Folder containing your GRIB files\n",
    "input_folder = '/media/lab/My Passport/hail/gfs/20190315/'\n",
    "# List of GRIB files to process\n",
    "grib_files = [\n",
    "    f\"gfs.0p25.2019031500.f{str(i).zfill(3)}.grib2\" for i in range(0, 25, 3)\n",
    "]\n",
    "# Path for the output NetCDF file\n",
    "output_file = '/media/lab/My Passport/hail/gfs/20190315/u_wind_all_levels1.nc'\n",
    "\n",
    "# Open multiple GRIB files and extract data for the u-component of wind at all isobaricInhPa levels\n",
    "datasets = []\n",
    "for grib_file in grib_files:\n",
    "    file_path = os.path.join(input_folder, grib_file)\n",
    "    try:\n",
    "        # Open dataset for u-component of wind ('u') at isobaricInhPa levels\n",
    "        ds = xr.open_dataset(file_path, engine='cfgrib', \n",
    "                             backend_kwargs={'filter_by_keys': {'typeOfLevel': 'isobaricInhPa', \n",
    "                                                                'shortName': 'u'}})\n",
    "        datasets.append(ds)\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping file {grib_file} due to error: {e}\")\n",
    "\n",
    "# Concatenate along the 'time' dimension if datasets were loaded successfully\n",
    "if datasets:\n",
    "    # Use 'minimal' for coords and 'override' for compatibility to handle duplicates or conflicts\n",
    "    combined_ds = xr.concat(datasets, dim='time', coords='minimal', compat='override')\n",
    "    \n",
    "    # Save to NetCDF\n",
    "    combined_ds.to_netcdf(output_file)\n",
    "    print(f\"Data successfully saved to {output_file}\")\n",
    "else:\n",
    "    print(\"No data extracted. Please check the input files or file paths.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8e75e4b4-3429-450f-aa7c-373d69e0ffeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully saved to /media/lab/My Passport/hail/gfs/20190315/v_wind_all_levels1.nc\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import os\n",
    "\n",
    "# Folder containing your GRIB files\n",
    "input_folder = '/media/lab/My Passport/hail/gfs/20190315/'\n",
    "# List of GRIB files to process\n",
    "grib_files = [\n",
    "    f\"gfs.0p25.2019031500.f{str(i).zfill(3)}.grib2\" for i in range(0, 25, 3)\n",
    "]\n",
    "# Path for the output NetCDF file\n",
    "output_file = '/media/lab/My Passport/hail/gfs/20190315/v_wind_all_levels1.nc'\n",
    "\n",
    "# Open multiple GRIB files and extract data for the u-component of wind at all isobaricInhPa levels\n",
    "datasets = []\n",
    "for grib_file in grib_files:\n",
    "    file_path = os.path.join(input_folder, grib_file)\n",
    "    ds = xr.open_dataset(file_path, engine='cfgrib', \n",
    "                         backend_kwargs={'filter_by_keys': {'typeOfLevel': 'isobaricInhPa', \n",
    "                                                            'shortName': 'v'}})\n",
    "    datasets.append(ds)\n",
    "\n",
    "# Concatenate all the datasets along the time dimension and save to NetCDF\n",
    "combined_ds = xr.concat(datasets, dim='time')\n",
    "combined_ds.to_netcdf(output_file)\n",
    "print(f\"Data successfully saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "279ac6fd-7dcd-4c05-87bd-0c77b03527fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully saved to /media/lab/My Passport/hail/gfs/v_wind_all_levels.nc\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import os\n",
    "\n",
    "# Folder containing your GRIB files\n",
    "input_folder = '/media/lab/My Passport/hail/gfs/20190315/'\n",
    "# List of GRIB files to process\n",
    "grib_files = [\n",
    "    f\"gfs.0p25.2019031500.f{str(i).zfill(3)}.grib2\" for i in range(0, 25, 3)\n",
    "]\n",
    "# Path for the output NetCDF file\n",
    "output_file = '/media/lab/My Passport/hail/gfs/20190315/v_wind_all_levels1.nc'\n",
    "\n",
    "# Open multiple GRIB files and extract data for the v-component of wind at all isobaricInhPa levels\n",
    "datasets = []\n",
    "for grib_file in grib_files:\n",
    "    file_path = os.path.join(input_folder, grib_file)\n",
    "    ds = xr.open_dataset(file_path, engine='cfgrib', \n",
    "                         backend_kwargs={'filter_by_keys': {'typeOfLevel': 'isobaricInhPa', \n",
    "                                                            'shortName': 'v'}})\n",
    "    datasets.append(ds)\n",
    "\n",
    "# Concatenate all the datasets along the time dimension and save to NetCDF\n",
    "combined_ds = xr.concat(datasets, dim='time')\n",
    "combined_ds.to_netcdf(output_file)\n",
    "print(f\"Data successfully saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ea8bd4d7-04f7-402b-ada7-8510c9ef781f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully saved to /media/lab/My Passport/hail/gfs/20190315/v_wind_all_levels1.nc\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import os\n",
    "\n",
    "# Folder containing your GRIB files\n",
    "input_folder = '/media/lab/My Passport/hail/gfs/20190315/'\n",
    "# List of GRIB files to process\n",
    "grib_files = [\n",
    "    f\"gfs.0p25.2019031500.f{str(i).zfill(3)}.grib2\" for i in range(0, 25, 3)\n",
    "]\n",
    "# Path for the output NetCDF file\n",
    "output_file = '/media/lab/My Passport/hail/gfs/20190315/v_wind_all_levels1.nc'\n",
    "\n",
    "# List to store datasets from each GRIB file\n",
    "datasets = []\n",
    "\n",
    "# Loop through each GRIB file to extract v-component wind data at all isobaricInhPa levels\n",
    "for grib_file in grib_files:\n",
    "    file_path = os.path.join(input_folder, grib_file)\n",
    "    try:\n",
    "        # Open dataset for v-component of wind ('v') at isobaricInhPa levels\n",
    "        ds = xr.open_dataset(file_path, engine='cfgrib', \n",
    "                             backend_kwargs={'filter_by_keys': {'typeOfLevel': 'isobaricInhPa', \n",
    "                                                                'shortName': 'v'}})\n",
    "        datasets.append(ds)\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping file {grib_file} due to error: {e}\")\n",
    "\n",
    "# Concatenate along the 'time' dimension if datasets were loaded successfully\n",
    "if datasets:\n",
    "    # Handle duplicate coordinates by setting `coords='minimal'` and `compat='override'`\n",
    "    combined_ds = xr.concat(datasets, dim='time', coords='minimal', compat='override')\n",
    "    \n",
    "    # Save to NetCDF\n",
    "    combined_ds.to_netcdf(output_file)\n",
    "    print(f\"Data successfully saved to {output_file}\")\n",
    "else:\n",
    "    print(\"No data extracted. Please check the input files or file paths.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5f787c8b-c8fd-4e48-926d-459f1dd4c5c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully saved to /media/lab/My Passport/hail/gfs/cin_pressureFromGroundLayer_all_levels.nc\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import os\n",
    "\n",
    "# Folder containing your GRIB files\n",
    "input_folder = '/media/lab/My Passport/hail/gfs/20190315/'\n",
    "# List of GRIB files to process\n",
    "grib_files = [\n",
    "    f\"gfs.0p25.2019031500.f{str(i).zfill(3)}.grib2\" for i in range(0, 25, 3)\n",
    "]\n",
    "# Path for the output NetCDF file\n",
    "output_file = '/media/lab/My Passport/hail/gfs/20190315/ci_wind_all_levels1.nc'\n",
    "\n",
    "# Open multiple GRIB files and extract CIN data at all pressureFromGroundLayer levels\n",
    "datasets = []\n",
    "for grib_file in grib_files:\n",
    "    file_path = os.path.join(input_folder, grib_file)\n",
    "    ds = xr.open_dataset(file_path, engine='cfgrib', \n",
    "                         backend_kwargs={'filter_by_keys': {'typeOfLevel': 'pressureFromGroundLayer', \n",
    "                                                            'shortName': 'cin'}})\n",
    "    datasets.append(ds)\n",
    "\n",
    "# Concatenate all the datasets along the time dimension and save to NetCDF\n",
    "combined_ds = xr.concat(datasets, dim='time')\n",
    "combined_ds.to_netcdf(output_file)\n",
    "print(f\"Data successfully saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d4ffd18a-8137-4c65-8bc3-d0b4799aae2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully saved to /media/lab/My Passport/hail/gfs/20190315/cin_all_levels.nc\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import os\n",
    "\n",
    "# Folder containing your GRIB files\n",
    "input_folder = '/media/lab/My Passport/hail/gfs/20190315/'\n",
    "# List of GRIB files to process\n",
    "grib_files = [\n",
    "    f\"gfs.0p25.2019031500.f{str(i).zfill(3)}.grib2\" for i in range(0, 25, 3)\n",
    "]\n",
    "# Path for the output NetCDF file\n",
    "output_file = '/media/lab/My Passport/hail/gfs/20190315/cin_all_levels.nc'\n",
    "\n",
    "# Initialize an empty list to collect the datasets\n",
    "datasets = []\n",
    "\n",
    "# Loop through each file and extract CIN data at all pressureFromGroundLayer levels\n",
    "for grib_file in grib_files:\n",
    "    file_path = os.path.join(input_folder, grib_file)\n",
    "    try:\n",
    "        # Open dataset for CIN at all pressureFromGroundLayer levels\n",
    "        ds = xr.open_dataset(\n",
    "            file_path,\n",
    "            engine='cfgrib',\n",
    "            backend_kwargs={'filter_by_keys': {'typeOfLevel': 'pressureFromGroundLayer', 'shortName': 'cin'}}\n",
    "        )\n",
    "        \n",
    "        # Append dataset if it contains the expected data\n",
    "        if 'cin' in ds.data_vars:\n",
    "            datasets.append(ds)\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping file {grib_file} due to error: {e}\")\n",
    "\n",
    "# Concatenate all datasets along the time dimension if any datasets were successfully loaded\n",
    "if datasets:\n",
    "    # Concatenate all datasets along the time dimension, preserving pressureFromGroundLayer levels\n",
    "    combined_ds = xr.concat(datasets, dim='time', coords='minimal', compat='override')\n",
    "    \n",
    "    # Save to NetCDF file\n",
    "    combined_ds.to_netcdf(output_file)\n",
    "    print(f\"Data successfully saved to {output_file}\")\n",
    "else:\n",
    "    print(\"No data extracted. Please check the input files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2fe99ece-db40-40cf-8542-29f5774c6ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully saved to /media/lab/My Passport/hail/gfs/cape_pressureFromGroundLayer_all_levels.nc\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import os\n",
    "\n",
    "# Folder containing your GRIB files\n",
    "input_folder = '/media/lab/My Passport/hail/gfs/20200303/'\n",
    "# List of GRIB files to process\n",
    "grib_files = [\n",
    "    f\"gfs.0p25.2020032300.f{str(i).zfill(3)}.grib2\" for i in range(0, 25, 3)\n",
    "]\n",
    "# Path for the output NetCDF file\n",
    "output_file = '/media/lab/My Passport/hail/gfs/cape_pressureFromGroundLayer_all_levels.nc'\n",
    "\n",
    "# Open multiple GRIB files and extract CAPE data at all pressureFromGroundLayer levels\n",
    "datasets = []\n",
    "for grib_file in grib_files:\n",
    "    file_path = os.path.join(input_foldeimport xarray as xr\n",
    "import os\n",
    "\n",
    "# Folder containing your GRIB files\n",
    "input_folder = '/media/lab/My Passport/hail/gfs/20200303/'\n",
    "# List of GRIB files to process\n",
    "grib_files = [\n",
    "    f\"gfs.0p25.2020032300.f{str(i).zfill(3)}.grib2\" for i in range(0, 25, 3)\n",
    "]\n",
    "# Path for the output NetCDF file\n",
    "output_file = '/media/lab/My Passport/hail/gfs/relative_humidity_isobaricInhPa_all_levels.nc'\n",
    "\n",
    "# Initialize an empty list to collect the datasets\n",
    "datasets = []\n",
    "\n",
    "# Loop through each file and extract only Relative Humidity data at isobaricInhPa levels\n",
    "for grib_file in grib_files:\n",
    "    file_path = os.path.join(input_folder, grib_file)\n",
    "    try:\n",
    "        # Open dataset for Relative Humidity at all isobaricInhPa levels\n",
    "        ds = xr.open_dataset(file_path, engine='cfgrib', \n",
    "                             backend_kwargs={'filter_by_keys': {'typeOfLevel': 'isobaricInhPa', \n",
    "                                                                'shortName': 'r'}})\n",
    "        # Append dataset if it contains the expected data\n",
    "        if 'r' in ds.data_vars:\n",
    "            datasets.append(ds)\n",
    "    except ValueError as e:\n",
    "        print(f\"Skipping file {grib_file} due to error: {e}\")\n",
    "\n",
    "# Concatenate all datasets along the time dimension if any datasets were successfully loaded\n",
    "if datasets:\n",
    "    combined_ds = xr.concat(datasets, dim='time', coords='minimal', compat='override')\n",
    "    combined_ds.to_netcdf(output_file)\n",
    "    print(f\"Data successfully saved to {output_file}\")\n",
    "else:\n",
    "    print(\"No data extracted. Please check the input files.\")\n",
    "r, grib_file)\n",
    "    ds = xr.open_dataset(file_path, engine='cfgrib', \n",
    "                         backend_kwargs={'filter_by_keys': {'typeOfLevel': 'pressureFromGroundLayer', \n",
    "                                                            'shortName': 'cape'}})\n",
    "    datasets.append(ds)\n",
    "\n",
    "# Concatenate all the datasets along the time dimension and save to NetCDF\n",
    "combined_ds = xr.concat(datasets, dim='time')\n",
    "combined_ds.to_netcdf(output_file)\n",
    "print(f\"Data successfully saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "35c462b1-a1f3-4dda-a0f8-643597b20d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAPE data successfully saved to /media/lab/My Passport/hail/gfs/20190315/cape_pressureFromGroundLayer_all_levels.nc\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import os\n",
    "\n",
    "# Folder containing your GRIB files\n",
    "input_folder = '/media/lab/My Passport/hail/gfs/20190315/'\n",
    "# List of GRIB files to process\n",
    "grib_files = [\n",
    "    f\"gfs.0p25.2019031500.f{str(i).zfill(3)}.grib2\" for i in range(0, 25, 3)\n",
    "]\n",
    "# Path for the output NetCDF file for CAPE data\n",
    "output_file_cape = '/media/lab/My Passport/hail/gfs/20190315/cape_pressureFromGroundLayer_all_levels.nc'\n",
    "\n",
    "# Initialize an empty list to collect the CAPE datasets\n",
    "datasets_cape = []\n",
    "\n",
    "# Loop through each GRIB file to extract CAPE data at all pressureFromGroundLayer levels\n",
    "for grib_file in grib_files:\n",
    "    file_path = os.path.join(input_folder, grib_file)\n",
    "    try:\n",
    "        # Open dataset for CAPE at all pressureFromGroundLayer levels\n",
    "        ds = xr.open_dataset(\n",
    "            file_path,\n",
    "            engine='cfgrib',\n",
    "            backend_kwargs={'filter_by_keys': {'typeOfLevel': 'pressureFromGroundLayer', 'shortName': 'cape'}}\n",
    "        )\n",
    "        \n",
    "        # Append dataset if it contains the expected data\n",
    "        if 'cape' in ds.data_vars:\n",
    "            datasets_cape.append(ds)\n",
    "    except ValueError as e:\n",
    "        print(f\"Skipping file {grib_file} due to error: {e}\")\n",
    "\n",
    "# Concatenate all datasets along the time dimension if any datasets were successfully loaded\n",
    "if datasets_cape:\n",
    "    combined_ds_cape = xr.concat(datasets_cape, dim='time', coords='minimal', compat='override')\n",
    "    combined_ds_cape.to_netcdf(output_file_cape)\n",
    "    print(f\"CAPE data successfully saved to {output_file_cape}\")\n",
    "else:\n",
    "    print(\"No CAPE data extracted. Please check the input files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0645682e-cb05-4c00-a5d4-d13f934d537c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully saved to /media/lab/My Passport/hail/gfs/20190315/relative_humidity_isobaricInhPa_all_levels.nc\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import os\n",
    "\n",
    "# Folder containing your GRIB files\n",
    "input_folder = '/media/lab/My Passport/hail/gfs/20190315/'\n",
    "# List of GRIB files to process\n",
    "grib_files = [\n",
    "    f\"gfs.0p25.2019031500.f{str(i).zfill(3)}.grib2\" for i in range(0, 25, 3)\n",
    "]\n",
    "# Path for the output NetCDF file\n",
    "output_file = '/media/lab/My Passport/hail/gfs/20190315/relative_humidity_isobaricInhPa_all_levels.nc'\n",
    "\n",
    "# Initialize an empty list to collect the datasets\n",
    "datasets = []\n",
    "\n",
    "# Loop through each file and extract Relative Humidity data at all isobaricInhPa levels\n",
    "for grib_file in grib_files:\n",
    "    file_path = os.path.join(input_folder, grib_file)\n",
    "    try:\n",
    "        # Open dataset for Relative Humidity at all isobaricInhPa levels\n",
    "        ds = xr.open_dataset(\n",
    "            file_path, \n",
    "            engine='cfgrib', \n",
    "            backend_kwargs={'filter_by_keys': {'typeOfLevel': 'isobaricInhPa', 'shortName': 'r'}}\n",
    "        )\n",
    "        \n",
    "        # Check and append dataset if it contains the expected data\n",
    "        if 'r' in ds.data_vars:\n",
    "            datasets.append(ds)\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping file {grib_file} due to error: {e}\")\n",
    "\n",
    "# Concatenate all datasets along the time dimension if any datasets were successfully loaded\n",
    "if datasets:\n",
    "    # Concatenate all datasets along the time dimension, preserving isobaric levels\n",
    "    combined_ds = xr.concat(datasets, dim='time', coords='minimal', compat='override')\n",
    "    \n",
    "    # Save to NetCDF file\n",
    "    combined_ds.to_netcdf(output_file)\n",
    "    print(f\"Data successfully saved to {output_file}\")\n",
    "else:\n",
    "    print(\"No data extracted. Please check the input files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1256ea28-d98f-4fb6-bb31-cff37114fe34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully saved to /media/lab/My Passport/hail/gfs/relative_humidity_isobaricInhPa_all_levels.nc\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import os\n",
    "\n",
    "# Folder containing your GRIB files\n",
    "input_folder = '/media/lab/My Passport/hail/gfs/20200303/'\n",
    "# List of GRIB files to process\n",
    "grib_files = [\n",
    "    f\"gfs.0p25.2020032300.f{str(i).zfill(3)}.grib2\" for i in range(0, 25, 3)\n",
    "]\n",
    "# Path for the output NetCDF file\n",
    "output_file = '/media/lab/My Passport/hail/gfs/relative_humidity_isobaricInhPa_all_levels.nc'\n",
    "\n",
    "# Initialize an empty list to collect the datasets\n",
    "datasets = []\n",
    "\n",
    "# Loop through each file and extract only Relative Humidity data at isobaricInhPa levels\n",
    "for grib_file in grib_files:\n",
    "    file_path = os.path.join(input_folder, grib_file)\n",
    "    try:\n",
    "        # Open dataset for Relative Humidity at all isobaricInhPa levels\n",
    "        ds = xr.open_dataset(file_path, engine='cfgrib', \n",
    "                             backend_kwargs={'filter_by_keys': {'typeOfLevel': 'isobaricInhPa', \n",
    "                                                                'shortName': 'r'}})\n",
    "        # Append dataset if it contains the expected data\n",
    "        if 'r' in ds.data_vars:\n",
    "            datasets.append(ds)\n",
    "    except ValueError as e:\n",
    "        print(f\"Skipping file {grib_file} due to error: {e}\")\n",
    "\n",
    "# Concatenate all datasets along the time dimension if any datasets were successfully loaded\n",
    "if datasets:\n",
    "    combined_ds = xr.concat(datasets, dim='time', coords='minimal', compat='override')\n",
    "    combined_ds.to_netcdf(output_file)\n",
    "    print(f\"Data successfully saved to {output_file}\")\n",
    "else:\n",
    "    print(\"No data extracted. Please check the input files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b3736a4b-41b1-48a6-9cb6-d3149eaaff0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully saved to /media/lab/My Passport/hail/gfs/specific_humidity_heightAboveGround_all_levels.nc\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import os\n",
    "\n",
    "# Folder containing your GRIB files\n",
    "input_folder = '/media/lab/My Passport/hail/gfs/20200303/'\n",
    "# List of GRIB files to process\n",
    "grib_files = [\n",
    "    f\"gfs.0p25.2020032300.f{str(i).zfill(3)}.grib2\" for i in range(0, 25, 3)\n",
    "]\n",
    "# Path for the output NetCDF file\n",
    "output_file = '/media/lab/My Passport/hail/gfs/specific_humidity_heightAboveGround_all_levels.nc'\n",
    "\n",
    "# Open multiple GRIB files and extract Specific Humidity data at all heightAboveGround levels\n",
    "datasets = []\n",
    "for grib_file in grib_files:\n",
    "    file_path = os.path.join(input_folder, grib_file)\n",
    "    ds = xr.open_dataset(file_path, engine='cfgrib', \n",
    "                         backend_kwargs={'filter_by_keys': {'typeOfLevel': 'heightAboveGround', \n",
    "                                                            'shortName': 'q'}})\n",
    "    datasets.append(ds)\n",
    "\n",
    "# Concatenate all the datasets along the time dimension and save to NetCDF\n",
    "combined_ds = xr.concat(datasets, dim='time')\n",
    "combined_ds.to_netcdf(output_file)\n",
    "print(f\"Data successfully saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "03634121-9fb4-4acb-9aa6-cc007afa6747",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'pressureFromGroundLayer' not present in all datasets and coords='different'. Either add 'pressureFromGroundLayer' to datasets where it is missing or specify coords='minimal'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m     datasets\u001b[38;5;241m.\u001b[39mappend(ds_height)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Concatenate all datasets along the time dimension and save to NetCDF\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m combined_ds \u001b[38;5;241m=\u001b[39m xr\u001b[38;5;241m.\u001b[39mconcat(datasets, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     34\u001b[0m combined_ds\u001b[38;5;241m.\u001b[39mto_netcdf(output_file)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData successfully saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/xarray/core/concat.py:252\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, dim, data_vars, coords, compat, positions, fill_value, join, combine_attrs)\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _dataarray_concat(\n\u001b[1;32m    241\u001b[0m         objs,\n\u001b[1;32m    242\u001b[0m         dim\u001b[38;5;241m=\u001b[39mdim,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    249\u001b[0m         combine_attrs\u001b[38;5;241m=\u001b[39mcombine_attrs,\n\u001b[1;32m    250\u001b[0m     )\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(first_obj, Dataset):\n\u001b[0;32m--> 252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _dataset_concat(\n\u001b[1;32m    253\u001b[0m         objs,\n\u001b[1;32m    254\u001b[0m         dim\u001b[38;5;241m=\u001b[39mdim,\n\u001b[1;32m    255\u001b[0m         data_vars\u001b[38;5;241m=\u001b[39mdata_vars,\n\u001b[1;32m    256\u001b[0m         coords\u001b[38;5;241m=\u001b[39mcoords,\n\u001b[1;32m    257\u001b[0m         compat\u001b[38;5;241m=\u001b[39mcompat,\n\u001b[1;32m    258\u001b[0m         positions\u001b[38;5;241m=\u001b[39mpositions,\n\u001b[1;32m    259\u001b[0m         fill_value\u001b[38;5;241m=\u001b[39mfill_value,\n\u001b[1;32m    260\u001b[0m         join\u001b[38;5;241m=\u001b[39mjoin,\n\u001b[1;32m    261\u001b[0m         combine_attrs\u001b[38;5;241m=\u001b[39mcombine_attrs,\n\u001b[1;32m    262\u001b[0m     )\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    265\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcan only concatenate xarray Dataset and DataArray \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    266\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjects, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(first_obj)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    267\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/xarray/core/concat.py:508\u001b[0m, in \u001b[0;36m_dataset_concat\u001b[0;34m(datasets, dim, data_vars, coords, compat, positions, fill_value, join, combine_attrs)\u001b[0m\n\u001b[1;32m    505\u001b[0m     datasets \u001b[38;5;241m=\u001b[39m [cast(T_Dataset, ds\u001b[38;5;241m.\u001b[39mexpand_dims(dim)) \u001b[38;5;28;01mfor\u001b[39;00m ds \u001b[38;5;129;01min\u001b[39;00m datasets]\n\u001b[1;32m    507\u001b[0m \u001b[38;5;66;03m# determine which variables to concatenate\u001b[39;00m\n\u001b[0;32m--> 508\u001b[0m concat_over, equals, concat_dim_lengths \u001b[38;5;241m=\u001b[39m _calc_concat_over(\n\u001b[1;32m    509\u001b[0m     datasets, dim, dim_names, data_vars, coords, compat\n\u001b[1;32m    510\u001b[0m )\n\u001b[1;32m    512\u001b[0m \u001b[38;5;66;03m# determine which variables to merge, and then merge them according to compat\u001b[39;00m\n\u001b[1;32m    513\u001b[0m variables_to_merge \u001b[38;5;241m=\u001b[39m (coord_names \u001b[38;5;241m|\u001b[39m data_names) \u001b[38;5;241m-\u001b[39m concat_over \u001b[38;5;241m-\u001b[39m unlabeled_dims\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/xarray/core/concat.py:409\u001b[0m, in \u001b[0;36m_calc_concat_over\u001b[0;34m(datasets, dim, dim_names, data_vars, coords, compat)\u001b[0m\n\u001b[1;32m    406\u001b[0m         concat_over\u001b[38;5;241m.\u001b[39mupdate(opt)\n\u001b[1;32m    408\u001b[0m process_subset_opt(data_vars, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_vars\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 409\u001b[0m process_subset_opt(coords, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoords\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m concat_over, equals, concat_dim_lengths\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/xarray/core/concat.py:341\u001b[0m, in \u001b[0;36m_calc_concat_over.<locals>.process_subset_opt\u001b[0;34m(opt, subset)\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(variables) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(datasets) \u001b[38;5;129;01mand\u001b[39;00m opt \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferent\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    342\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m not present in all datasets and coords=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferent\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    343\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEither add \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m to datasets where it is missing or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    344\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspecify coords=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminimal\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    345\u001b[0m     )\n\u001b[1;32m    347\u001b[0m \u001b[38;5;66;03m# first check without comparing values i.e. no computes\u001b[39;00m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m var \u001b[38;5;129;01min\u001b[39;00m variables[\u001b[38;5;241m1\u001b[39m:]:\n",
      "\u001b[0;31mValueError\u001b[0m: 'pressureFromGroundLayer' not present in all datasets and coords='different'. Either add 'pressureFromGroundLayer' to datasets where it is missing or specify coords='minimal'."
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import os\n",
    "\n",
    "# Folder containing your GRIB files\n",
    "input_folder = '/media/lab/My Passport/hail/gfs/20200303/'\n",
    "# List of GRIB files to process\n",
    "grib_files = [\n",
    "    f\"gfs.0p25.2020032300.f{str(i).zfill(3)}.grib2\" for i in range(0, 25, 3)\n",
    "]\n",
    "# Path for the output NetCDF file\n",
    "output_file = '/media/lab/My Passport/hail/gfs/specific_humidity_multiple_levels.nc'\n",
    "\n",
    "# Open multiple GRIB files and extract Specific Humidity data at both level types\n",
    "datasets = []\n",
    "for grib_file in grib_files:\n",
    "    file_path = os.path.join(input_folder, grib_file)\n",
    "    \n",
    "    # Extract Specific Humidity at all pressureFromGroundLayer levels\n",
    "    ds_pressure = xr.open_dataset(file_path, engine='cfgrib', \n",
    "                                  backend_kwargs={'filter_by_keys': {'typeOfLevel': 'pressureFromGroundLayer', \n",
    "                                                                     'shortName': 'q'}})\n",
    "    \n",
    "    # Extract Specific Humidity at all heightAboveGround levels\n",
    "    ds_height = xr.open_dataset(file_path, engine='cfgrib', \n",
    "                                backend_kwargs={'filter_by_keys': {'typeOfLevel': 'heightAboveGround', \n",
    "                                                                   'shortName': 'q'}})\n",
    "    \n",
    "    # Append datasets for both level types\n",
    "    datasets.append(ds_pressure)\n",
    "    datasets.append(ds_height)\n",
    "\n",
    "# Concatenate all datasets along the time dimension and save to NetCDF\n",
    "combined_ds = xr.concat(datasets, dim='time')\n",
    "combined_ds.to_netcdf(output_file)\n",
    "print(f\"Data successfully saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6fb795a8-61c0-4251-9090-e09a71e6cc16",
   "metadata": {},
   "outputs": [
    {
     "ename": "MergeError",
     "evalue": "conflicting values for variable 'step' on objects to be combined. You can skip this check by specifying compat='override'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMergeError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 44\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Check if we have any datasets to concatenate\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m datasets:\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m# Concatenate all datasets along the time dimension and save to NetCDF\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m     combined_ds \u001b[38;5;241m=\u001b[39m xr\u001b[38;5;241m.\u001b[39mconcat(datasets, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m, coords\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminimal\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Use minimal coords\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     combined_ds\u001b[38;5;241m.\u001b[39mto_netcdf(output_file)\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData successfully saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/xarray/core/concat.py:252\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, dim, data_vars, coords, compat, positions, fill_value, join, combine_attrs)\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _dataarray_concat(\n\u001b[1;32m    241\u001b[0m         objs,\n\u001b[1;32m    242\u001b[0m         dim\u001b[38;5;241m=\u001b[39mdim,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    249\u001b[0m         combine_attrs\u001b[38;5;241m=\u001b[39mcombine_attrs,\n\u001b[1;32m    250\u001b[0m     )\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(first_obj, Dataset):\n\u001b[0;32m--> 252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _dataset_concat(\n\u001b[1;32m    253\u001b[0m         objs,\n\u001b[1;32m    254\u001b[0m         dim\u001b[38;5;241m=\u001b[39mdim,\n\u001b[1;32m    255\u001b[0m         data_vars\u001b[38;5;241m=\u001b[39mdata_vars,\n\u001b[1;32m    256\u001b[0m         coords\u001b[38;5;241m=\u001b[39mcoords,\n\u001b[1;32m    257\u001b[0m         compat\u001b[38;5;241m=\u001b[39mcompat,\n\u001b[1;32m    258\u001b[0m         positions\u001b[38;5;241m=\u001b[39mpositions,\n\u001b[1;32m    259\u001b[0m         fill_value\u001b[38;5;241m=\u001b[39mfill_value,\n\u001b[1;32m    260\u001b[0m         join\u001b[38;5;241m=\u001b[39mjoin,\n\u001b[1;32m    261\u001b[0m         combine_attrs\u001b[38;5;241m=\u001b[39mcombine_attrs,\n\u001b[1;32m    262\u001b[0m     )\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    265\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcan only concatenate xarray Dataset and DataArray \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    266\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjects, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(first_obj)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    267\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/xarray/core/concat.py:524\u001b[0m, in \u001b[0;36m_dataset_concat\u001b[0;34m(datasets, dim, data_vars, coords, compat, positions, fill_value, join, combine_attrs)\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m variables_to_merge:\n\u001b[1;32m    519\u001b[0m     grouped \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    520\u001b[0m         k: v\n\u001b[1;32m    521\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m collect_variables_and_indexes(datasets)\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    522\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m variables_to_merge\n\u001b[1;32m    523\u001b[0m     }\n\u001b[0;32m--> 524\u001b[0m     merged_vars, merged_indexes \u001b[38;5;241m=\u001b[39m merge_collected(\n\u001b[1;32m    525\u001b[0m         grouped, compat\u001b[38;5;241m=\u001b[39mcompat, equals\u001b[38;5;241m=\u001b[39mequals\n\u001b[1;32m    526\u001b[0m     )\n\u001b[1;32m    527\u001b[0m     result_vars\u001b[38;5;241m.\u001b[39mupdate(merged_vars)\n\u001b[1;32m    528\u001b[0m     result_indexes\u001b[38;5;241m.\u001b[39mupdate(merged_indexes)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/xarray/core/merge.py:291\u001b[0m, in \u001b[0;36mmerge_collected\u001b[0;34m(grouped, prioritized, compat, combine_attrs, equals)\u001b[0m\n\u001b[1;32m    289\u001b[0m variables \u001b[38;5;241m=\u001b[39m [variable \u001b[38;5;28;01mfor\u001b[39;00m variable, _ \u001b[38;5;129;01min\u001b[39;00m elements_list]\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 291\u001b[0m     merged_vars[name] \u001b[38;5;241m=\u001b[39m unique_variable(\n\u001b[1;32m    292\u001b[0m         name, variables, compat, equals\u001b[38;5;241m.\u001b[39mget(name, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    293\u001b[0m     )\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m MergeError:\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compat \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminimal\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    296\u001b[0m         \u001b[38;5;66;03m# we need more than \"minimal\" compatibility (for which\u001b[39;00m\n\u001b[1;32m    297\u001b[0m         \u001b[38;5;66;03m# we drop conflicting coordinates)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/xarray/core/merge.py:145\u001b[0m, in \u001b[0;36munique_variable\u001b[0;34m(name, variables, compat, equals)\u001b[0m\n\u001b[1;32m    142\u001b[0m                 \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m equals:\n\u001b[0;32m--> 145\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MergeError(\n\u001b[1;32m    146\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconflicting values for variable \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m on objects to be combined. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    147\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can skip this check by specifying compat=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moverride\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    148\u001b[0m     )\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m combine_method:\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m var \u001b[38;5;129;01min\u001b[39;00m variables[\u001b[38;5;241m1\u001b[39m:]:\n",
      "\u001b[0;31mMergeError\u001b[0m: conflicting values for variable 'step' on objects to be combined. You can skip this check by specifying compat='override'."
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import os\n",
    "\n",
    "# Folder containing your GRIB files\n",
    "input_folder = '/media/lab/My Passport/hail/gfs/20200303/'\n",
    "# List of GRIB files to process\n",
    "grib_files = [\n",
    "    f\"gfs.0p25.2020032300.f{str(i).zfill(3)}.grib2\" for i in range(0, 25, 3)\n",
    "]\n",
    "# Path for the output NetCDF file\n",
    "output_file = '/media/lab/My Passport/hail/gfs/specific_humidity_multiple_levels.nc'\n",
    "\n",
    "# Initialize an empty list to hold all datasets\n",
    "datasets = []\n",
    "\n",
    "for grib_file in grib_files:\n",
    "    file_path = os.path.join(input_folder, grib_file)\n",
    "    \n",
    "    # Extract Specific Humidity at all pressureFromGroundLayer levels\n",
    "    try:\n",
    "        ds_pressure = xr.open_dataset(file_path, engine='cfgrib', \n",
    "                                      backend_kwargs={'filter_by_keys': {'typeOfLevel': 'pressureFromGroundLayer', \n",
    "                                                                         'shortName': 'q'}})\n",
    "        # Only append if the dataset has data\n",
    "        if 'q' in ds_pressure.data_vars:\n",
    "            datasets.append(ds_pressure)\n",
    "    except ValueError as e:\n",
    "        print(f\"Warning: {e} for file {grib_file} - skipping pressureFromGroundLayer extraction.\")\n",
    "\n",
    "    # Extract Specific Humidity at all heightAboveGround levels\n",
    "    try:\n",
    "        ds_height = xr.open_dataset(file_path, engine='cfgrib', \n",
    "                                    backend_kwargs={'filter_by_keys': {'typeOfLevel': 'heightAboveGround', \n",
    "                                                                       'shortName': 'q'}})\n",
    "        # Only append if the dataset has data\n",
    "        if 'q' in ds_height.data_vars:\n",
    "            datasets.append(ds_height)\n",
    "    except ValueError as e:\n",
    "        print(f\"Warning: {e} for file {grib_file} - skipping heightAboveGround extraction.\")\n",
    "\n",
    "# Check if we have any datasets to concatenate\n",
    "if datasets:\n",
    "    # Concatenate all datasets along the time dimension and save to NetCDF\n",
    "    combined_ds = xr.concat(datasets, dim='time', coords='minimal')  # Use minimal coords\n",
    "    combined_ds.to_netcdf(output_file)\n",
    "    print(f\"Data successfully saved to {output_file}\")\n",
    "else:\n",
    "    print(\"No data extracted. Please check the input files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "75562597-776c-4a49-bab8-c0029ca9615e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully saved to /media/lab/My Passport/hail/gfs/20190315/specific_humidity_multiple_levels2.nc\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import os\n",
    "\n",
    "# Folder containing your GRIB files\n",
    "input_folder = '/media/lab/My Passport/hail/gfs/20190315/'\n",
    "# List of GRIB files to process\n",
    "grib_files = [\n",
    "    f\"gfs.0p25.2019031500.f{str(i).zfill(3)}.grib2\" for i in range(0, 25, 3)\n",
    "]\n",
    "# Path for the output NetCDF file\n",
    "output_file = '/media/lab/My Passport/hail/gfs/20190315/specific_humidity_multiple_levels2.nc'\n",
    "\n",
    "# Initialize an empty list to hold all datasets\n",
    "datasets = []\n",
    "\n",
    "for grib_file in grib_files:\n",
    "    file_path = os.path.join(input_folder, grib_file)\n",
    "    \n",
    "    # Extract Specific Humidity at all pressureFromGroundLayer levels\n",
    "    try:\n",
    "        ds_pressure = xr.open_dataset(file_path, engine='cfgrib', \n",
    "                                      backend_kwargs={'filter_by_keys': {'typeOfLevel': 'pressureFromGroundLayer', \n",
    "                                                                         'shortName': 'q'}})\n",
    "        # Only append if the dataset has data\n",
    "        if 'q' in ds_pressure.data_vars:\n",
    "            datasets.append(ds_pressure)\n",
    "    except ValueError as e:\n",
    "        print(f\"Warning: {e} for file {grib_file} - skipping pressureFromGroundLayer extraction.\")\n",
    "\n",
    "    # Extract Specific Humidity at all heightAboveGround levels\n",
    "    try:\n",
    "        ds_height = xr.open_dataset(file_path, engine='cfgrib', \n",
    "                                    backend_kwargs={'filter_by_keys': {'typeOfLevel': 'heightAboveGround', \n",
    "                                                                       'shortName': 'q'}})\n",
    "        # Only append if the dataset has data\n",
    "        if 'q' in ds_height.data_vars:\n",
    "            datasets.append(ds_height)\n",
    "    except ValueError as e:\n",
    "        print(f\"Warning: {e} for file {grib_file} - skipping heightAboveGround extraction.\")\n",
    "\n",
    "# Check if we have any datasets to concatenate\n",
    "if datasets:\n",
    "    # Concatenate all datasets along the time dimension and save to NetCDF\n",
    "    combined_ds = xr.concat(datasets, dim='time', coords='minimal', compat='override')  # Use compat='override'\n",
    "    combined_ds.to_netcdf(output_file)\n",
    "    print(f\"Data successfully saved to {output_file}\")\n",
    "else:\n",
    "    print(\"No data extracted. Please check the input files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "17980df3-5720-4057-b740-427f7df34be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully saved to /media/lab/My Passport/hail/gfs/20190315/specific_humidity_heightAboveGround_levels.nc\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import os\n",
    "\n",
    "# Folder containing your GRIB files\n",
    "input_folder = '/media/lab/My Passport/hail/gfs/20190315/'\n",
    "# List of GRIB files to process\n",
    "grib_files = [\n",
    "    f\"gfs.0p25.2019031500.f{str(i).zfill(3)}.grib2\" for i in range(0, 25, 3)\n",
    "]\n",
    "# Path for the output NetCDF file\n",
    "output_file = '/media/lab/My Passport/hail/gfs/20190315/specific_humidity_heightAboveGround_levels.nc'\n",
    "\n",
    "# Initialize an empty list to collect the datasets\n",
    "datasets = []\n",
    "\n",
    "# Loop through each file and extract Specific Humidity data at all heightAboveGround levels\n",
    "for grib_file in grib_files:\n",
    "    file_path = os.path.join(input_folder, grib_file)\n",
    "    try:\n",
    "        # Open dataset for Specific Humidity at all heightAboveGround levels\n",
    "        ds = xr.open_dataset(\n",
    "            file_path,\n",
    "            engine='cfgrib',\n",
    "            backend_kwargs={'filter_by_keys': {'typeOfLevel': 'heightAboveGround', 'shortName': 'q'}}\n",
    "        )\n",
    "        \n",
    "        # Append dataset if it contains the expected data\n",
    "        if 'q' in ds.data_vars:\n",
    "            datasets.append(ds)\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping file {grib_file} due to error: {e}\")\n",
    "\n",
    "# Concatenate all datasets along the time dimension if any datasets were successfully loaded\n",
    "if datasets:\n",
    "    # Concatenate all datasets along the time dimension, preserving heightAboveGround levels\n",
    "    combined_ds = xr.concat(datasets, dim='time', coords='minimal', compat='override')\n",
    "    \n",
    "    # Save to NetCDF file\n",
    "    combined_ds.to_netcdf(output_file)\n",
    "    print(f\"Data successfully saved to {output_file}\")\n",
    "else:\n",
    "    print(\"No data extracted. Please check the input files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0cfaf5d8-7314-4fdc-b947-8d6ab7de0dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping file gfs.0p25.2019031500.f000.grib2 due to error: \"'heightAboveGround' is not a valid dimension or coordinate\"\n",
      "Skipping file gfs.0p25.2019031500.f003.grib2 due to error: \"'heightAboveGround' is not a valid dimension or coordinate\"\n",
      "Skipping file gfs.0p25.2019031500.f006.grib2 due to error: \"'heightAboveGround' is not a valid dimension or coordinate\"\n",
      "Skipping file gfs.0p25.2019031500.f009.grib2 due to error: \"'heightAboveGround' is not a valid dimension or coordinate\"\n",
      "Skipping file gfs.0p25.2019031500.f012.grib2 due to error: \"'heightAboveGround' is not a valid dimension or coordinate\"\n",
      "Skipping file gfs.0p25.2019031500.f015.grib2 due to error: \"'heightAboveGround' is not a valid dimension or coordinate\"\n",
      "Skipping file gfs.0p25.2019031500.f018.grib2 due to error: \"'heightAboveGround' is not a valid dimension or coordinate\"\n",
      "Skipping file gfs.0p25.2019031500.f021.grib2 due to error: \"'heightAboveGround' is not a valid dimension or coordinate\"\n",
      "Skipping file gfs.0p25.2019031500.f024.grib2 due to error: \"'heightAboveGround' is not a valid dimension or coordinate\"\n",
      "No data extracted. Please check the input files.\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import os\n",
    "\n",
    "# Folder containing your GRIB files\n",
    "input_folder = '/media/lab/My Passport/hail/gfs/20190315/'\n",
    "# List of GRIB files to process\n",
    "grib_files = [\n",
    "    f\"gfs.0p25.2019031500.f{str(i).zfill(3)}.grib2\" for i in range(0, 25, 3)\n",
    "]\n",
    "# Path for the output NetCDF file\n",
    "output_file = '/media/lab/My Passport/hail/gfs/20190315/temperature_heightAboveGround_2m.nc'\n",
    "\n",
    "# Initialize an empty list to collect the datasets\n",
    "datasets = []\n",
    "\n",
    "# Loop through each file and extract Temperature data at heightAboveGround level of 2m\n",
    "for grib_file in grib_files:\n",
    "    file_path = os.path.join(input_folder, grib_file)\n",
    "    try:\n",
    "        # Open dataset for Temperature at 2 meters above ground\n",
    "        ds = xr.open_dataset(\n",
    "            file_path,\n",
    "            engine='cfgrib',\n",
    "            backend_kwargs={'filter_by_keys': {'typeOfLevel': 'heightAboveGround', 'shortName': 't2m'}}\n",
    "        )\n",
    "        \n",
    "        # Filter to select only Temperature at 2 meters\n",
    "        ds_filtered = ds.sel(heightAboveGround=2)\n",
    "        \n",
    "        # Append dataset if it contains the expected data\n",
    "        if 't' in ds_filtered.data_vars:\n",
    "            datasets.append(ds_filtered)\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping file {grib_file} due to error: {e}\")\n",
    "\n",
    "# Concatenate all datasets along the time dimension if any datasets were successfully loaded\n",
    "if datasets:\n",
    "    # Concatenate all datasets along the time dimension, preserving heightAboveGround levels\n",
    "    combined_ds = xr.concat(datasets, dim='time', coords='minimal', compat='override')\n",
    "    \n",
    "    # Save to NetCDF file\n",
    "    combined_ds.to_netcdf(output_file)\n",
    "    print(f\"Data successfully saved to {output_file}\")\n",
    "else:\n",
    "    print(\"No data extracted. Please check the input files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7bbb225a-0fb8-42ec-bb3c-560b27882289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping file gfs.0p25.2019031500.f000.grib2 due to error: \"not all values found in index 'heightAboveGround'. Try setting the `method` keyword argument (example: method='nearest').\"\n",
      "Skipping file gfs.0p25.2019031500.f003.grib2 due to error: \"not all values found in index 'heightAboveGround'. Try setting the `method` keyword argument (example: method='nearest').\"\n",
      "Skipping file gfs.0p25.2019031500.f006.grib2 due to error: \"not all values found in index 'heightAboveGround'. Try setting the `method` keyword argument (example: method='nearest').\"\n",
      "Skipping file gfs.0p25.2019031500.f009.grib2 due to error: \"not all values found in index 'heightAboveGround'. Try setting the `method` keyword argument (example: method='nearest').\"\n",
      "Skipping file gfs.0p25.2019031500.f012.grib2 due to error: \"not all values found in index 'heightAboveGround'. Try setting the `method` keyword argument (example: method='nearest').\"\n",
      "Skipping file gfs.0p25.2019031500.f015.grib2 due to error: \"not all values found in index 'heightAboveGround'. Try setting the `method` keyword argument (example: method='nearest').\"\n",
      "Skipping file gfs.0p25.2019031500.f018.grib2 due to error: \"not all values found in index 'heightAboveGround'. Try setting the `method` keyword argument (example: method='nearest').\"\n",
      "Skipping file gfs.0p25.2019031500.f021.grib2 due to error: \"not all values found in index 'heightAboveGround'. Try setting the `method` keyword argument (example: method='nearest').\"\n",
      "Skipping file gfs.0p25.2019031500.f024.grib2 due to error: \"not all values found in index 'heightAboveGround'. Try setting the `method` keyword argument (example: method='nearest').\"\n",
      "No data extracted. Please check the input files.\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import os\n",
    "\n",
    "# Folder containing your GRIB files\n",
    "input_folder = '/media/lab/My Passport/hail/gfs/20190315/'\n",
    "# List of GRIB files to process\n",
    "grib_files = [\n",
    "    f\"gfs.0p25.2019031500.f{str(i).zfill(3)}.grib2\" for i in range(0, 25, 3)\n",
    "]\n",
    "# Path for the output NetCDF file\n",
    "output_file = '/media/lab/My Passport/hail/gfs/20190315/temperature_heightAboveGround_2m.nc'\n",
    "\n",
    "# Initialize an empty list to collect the datasets\n",
    "datasets = []\n",
    "\n",
    "# Loop through each file and extract Temperature data at heightAboveGround level of 2m\n",
    "for grib_file in grib_files:\n",
    "    file_path = os.path.join(input_folder, grib_file)\n",
    "    try:\n",
    "        # Open dataset for Temperature at 2 meters above ground\n",
    "        ds = xr.open_dataset(\n",
    "            file_path,\n",
    "            engine='cfgrib',\n",
    "            backend_kwargs={'filter_by_keys': {'typeOfLevel': 'heightAboveGround', 'shortName': 't'}}\n",
    "        )\n",
    "        \n",
    "        # Select only the Temperature variable at the 2-meter level\n",
    "        ds_filtered = ds.sel(heightAboveGround=2)\n",
    "        \n",
    "        # Append dataset if it contains the expected data\n",
    "        if 't' in ds_filtered.data_vars:\n",
    "            datasets.append(ds_filtered)\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping file {grib_file} due to error: {e}\")\n",
    "\n",
    "# Concatenate all datasets along the time dimension if any datasets were successfully loaded\n",
    "if datasets:\n",
    "    # Concatenate all datasets along the time dimension, preserving heightAboveGround levels\n",
    "    combined_ds = xr.concat(datasets, dim='time', coords='minimal', compat='override')\n",
    "    \n",
    "    # Save to NetCDF file\n",
    "    combined_ds.to_netcdf(output_file)\n",
    "    print(f\"Data successfully saved to {output_file}\")\n",
    "else:\n",
    "    print(\"No data extracted. Please check the input files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1ed80082-d43b-47c9-b42e-6f0e2264aa45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Can't create file '/media/lab/My Passport/hail/gfs/20200303/gfs.0p25.2019031500.f000.grib2.47d85.idx'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lab/anaconda3/lib/python3.11/site-packages/cfgrib/messages.py\", line 538, in from_indexpath_or_filestream\n",
      "    self = cls.from_fieldset(filestream, index_keys, computed_keys)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lab/anaconda3/lib/python3.11/site-packages/cfgrib/messages.py\", line 378, in from_fieldset\n",
      "    return cls.from_fieldset_and_iteritems(fieldset, iteritems, index_keys, computed_keys)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lab/anaconda3/lib/python3.11/site-packages/cfgrib/messages.py\", line 391, in from_fieldset_and_iteritems\n",
      "    for field_id, raw_field in iteritems:\n",
      "  File \"/home/lab/anaconda3/lib/python3.11/site-packages/cfgrib/messages.py\", line 291, in __iter__\n",
      "    for message in self.itervalues():\n",
      "  File \"/home/lab/anaconda3/lib/python3.11/site-packages/cfgrib/messages.py\", line 267, in itervalues\n",
      "    with open(self.filestream.path, \"rb\") as file:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/media/lab/My Passport/hail/gfs/20200303/gfs.0p25.2019031500.f000.grib2'\n",
      "Can't read index file '/media/lab/My Passport/hail/gfs/20200303/gfs.0p25.2019031500.f000.grib2.47d85.idx'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lab/anaconda3/lib/python3.11/site-packages/cfgrib/messages.py\", line 547, in from_indexpath_or_filestream\n",
      "    index_mtime = os.path.getmtime(indexpath)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"<frozen genericpath>\", line 55, in getmtime\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/media/lab/My Passport/hail/gfs/20200303/gfs.0p25.2019031500.f000.grib2.47d85.idx'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping file gfs.0p25.2019031500.f000.grib2 due to error: [Errno 2] No such file or directory: '/media/lab/My Passport/hail/gfs/20200303/gfs.0p25.2019031500.f000.grib2'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Can't create file '/media/lab/My Passport/hail/gfs/20200303/gfs.0p25.2019031500.f003.grib2.47d85.idx'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lab/anaconda3/lib/python3.11/site-packages/cfgrib/messages.py\", line 538, in from_indexpath_or_filestream\n",
      "    self = cls.from_fieldset(filestream, index_keys, computed_keys)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lab/anaconda3/lib/python3.11/site-packages/cfgrib/messages.py\", line 378, in from_fieldset\n",
      "    return cls.from_fieldset_and_iteritems(fieldset, iteritems, index_keys, computed_keys)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lab/anaconda3/lib/python3.11/site-packages/cfgrib/messages.py\", line 391, in from_fieldset_and_iteritems\n",
      "    for field_id, raw_field in iteritems:\n",
      "  File \"/home/lab/anaconda3/lib/python3.11/site-packages/cfgrib/messages.py\", line 291, in __iter__\n",
      "    for message in self.itervalues():\n",
      "  File \"/home/lab/anaconda3/lib/python3.11/site-packages/cfgrib/messages.py\", line 267, in itervalues\n",
      "    with open(self.filestream.path, \"rb\") as file:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/media/lab/My Passport/hail/gfs/20200303/gfs.0p25.2019031500.f003.grib2'\n",
      "Can't read index file '/media/lab/My Passport/hail/gfs/20200303/gfs.0p25.2019031500.f003.grib2.47d85.idx'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lab/anaconda3/lib/python3.11/site-packages/cfgrib/messages.py\", line 547, in from_indexpath_or_filestream\n",
      "    index_mtime = os.path.getmtime(indexpath)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"<frozen genericpath>\", line 55, in getmtime\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/media/lab/My Passport/hail/gfs/20200303/gfs.0p25.2019031500.f003.grib2.47d85.idx'\n",
      "Can't create file '/media/lab/My Passport/hail/gfs/20200303/gfs.0p25.2019031500.f006.grib2.47d85.idx'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lab/anaconda3/lib/python3.11/site-packages/cfgrib/messages.py\", line 538, in from_indexpath_or_filestream\n",
      "    self = cls.from_fieldset(filestream, index_keys, computed_keys)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lab/anaconda3/lib/python3.11/site-packages/cfgrib/messages.py\", line 378, in from_fieldset\n",
      "    return cls.from_fieldset_and_iteritems(fieldset, iteritems, index_keys, computed_keys)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lab/anaconda3/lib/python3.11/site-packages/cfgrib/messages.py\", line 391, in from_fieldset_and_iteritems\n",
      "    for field_id, raw_field in iteritems:\n",
      "  File \"/home/lab/anaconda3/lib/python3.11/site-packages/cfgrib/messages.py\", line 291, in __iter__\n",
      "    for message in self.itervalues():\n",
      "  File \"/home/lab/anaconda3/lib/python3.11/site-packages/cfgrib/messages.py\", line 267, in itervalues\n",
      "    with open(self.filestream.path, \"rb\") as file:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/media/lab/My Passport/hail/gfs/20200303/gfs.0p25.2019031500.f006.grib2'\n",
      "Can't read index file '/media/lab/My Passport/hail/gfs/20200303/gfs.0p25.2019031500.f006.grib2.47d85.idx'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lab/anaconda3/lib/python3.11/site-packages/cfgrib/messages.py\", line 547, in from_indexpath_or_filestream\n",
      "    index_mtime = os.path.getmtime(indexpath)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"<frozen genericpath>\", line 55, in getmtime\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/media/lab/My Passport/hail/gfs/20200303/gfs.0p25.2019031500.f006.grib2.47d85.idx'\n",
      "Can't create file '/media/lab/My Passport/hail/gfs/20200303/gfs.0p25.2019031500.f009.grib2.47d85.idx'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lab/anaconda3/lib/python3.11/site-packages/cfgrib/messages.py\", line 538, in from_indexpath_or_filestream\n",
      "    self = cls.from_fieldset(filestream, index_keys, computed_keys)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lab/anaconda3/lib/python3.11/site-packages/cfgrib/messages.py\", line 378, in from_fieldset\n",
      "    return cls.from_fieldset_and_iteritems(fieldset, iteritems, index_keys, computed_keys)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lab/anaconda3/lib/python3.11/site-packages/cfgrib/messages.py\", line 391, in from_fieldset_and_iteritems\n",
      "    for field_id, raw_field in iteritems:\n",
      "  File \"/home/lab/anaconda3/lib/python3.11/site-packages/cfgrib/messages.py\", line 291, in __iter__\n",
      "    for message in self.itervalues():\n",
      "  File \"/home/lab/anaconda3/lib/python3.11/site-packages/cfgrib/messages.py\", line 267, in itervalues\n",
      "    with open(self.filestream.path, \"rb\") as file:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/media/lab/My Passport/hail/gfs/20200303/gfs.0p25.2019031500.f009.grib2'\n",
      "Can't read index file '/media/lab/My Passport/hail/gfs/20200303/gfs.0p25.2019031500.f009.grib2.47d85.idx'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lab/anaconda3/lib/python3.11/site-packages/cfgrib/messages.py\", line 547, in from_indexpath_or_filestream\n",
      "    index_mtime = os.path.getmtime(indexpath)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"<frozen genericpath>\", line 55, in getmtime\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/media/lab/My Passport/hail/gfs/20200303/gfs.0p25.2019031500.f009.grib2.47d85.idx'\n",
      "Can't create file '/media/lab/My Passport/hail/gfs/20200303/gfs.0p25.2019031500.f012.grib2.47d85.idx'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lab/anaconda3/lib/python3.11/site-packages/cfgrib/messages.py\", line 538, in from_indexpath_or_filestream\n",
      "    self = cls.from_fieldset(filestream, index_keys, computed_keys)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lab/anaconda3/lib/python3.11/site-packages/cfgrib/messages.py\", line 378, in from_fieldset\n",
      "    return cls.from_fieldset_and_iteritems(fieldset, iteritems, index_keys, computed_keys)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lab/anaconda3/lib/python3.11/site-packages/cfgrib/messages.py\", line 391, in from_fieldset_and_iteritems\n",
      "    for field_id, raw_field in iteritems:\n",
      "  File \"/home/lab/anaconda3/lib/python3.11/site-packages/cfgrib/messages.py\", line 291, in __iter__\n",
      "    for message in self.itervalues():\n",
      "  File \"/home/lab/anaconda3/lib/python3.11/site-packages/cfgrib/messages.py\", line 267, in itervalues\n",
      "    with open(self.filestream.path, \"rb\") as file:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/media/lab/My Passport/hail/gfs/20200303/gfs.0p25.2019031500.f012.grib2'\n",
      "Can't read index file '/media/lab/My Passport/hail/gfs/20200303/gfs.0p25.2019031500.f012.grib2.47d85.idx'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lab/anaconda3/lib/python3.11/site-packages/cfgrib/messages.py\", line 547, in from_indexpath_or_filestream\n",
      "    index_mtime = os.path.getmtime(indexpath)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"<frozen genericpath>\", line 55, in getmtime\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/media/lab/My Passport/hail/gfs/20200303/gfs.0p25.2019031500.f012.grib2.47d85.idx'\n",
      "Can't create file '/media/lab/My Passport/hail/gfs/20200303/gfs.0p25.2019031500.f015.grib2.47d85.idx'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lab/anaconda3/lib/python3.11/site-packages/cfgrib/messages.py\", line 538, in from_indexpath_or_filestream\n",
      "    self = cls.from_fieldset(filestream, index_keys, computed_keys)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lab/anaconda3/lib/python3.11/site-packages/cfgrib/messages.py\", line 378, in from_fieldset\n",
      "    return cls.from_fieldset_and_iteritems(fieldset, iteritems, index_keys, computed_keys)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lab/anaconda3/lib/python3.11/site-packages/cfgrib/messages.py\", line 391, in from_fieldset_and_iteritems\n",
      "    for field_id, raw_field in iteritems:\n",
      "  File \"/home/lab/anaconda3/lib/python3.11/site-packages/cfgrib/messages.py\", line 291, in __iter__\n",
      "    for message in self.itervalues():\n",
      "  File \"/home/lab/anaconda3/lib/python3.11/site-packages/cfgrib/messages.py\", line 267, in itervalues\n",
      "    with open(self.filestream.path, \"rb\") as file:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/media/lab/My Passport/hail/gfs/20200303/gfs.0p25.2019031500.f015.grib2'\n",
      "Can't read index file '/media/lab/My Passport/hail/gfs/20200303/gfs.0p25.2019031500.f015.grib2.47d85.idx'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lab/anaconda3/lib/python3.11/site-packages/cfgrib/messages.py\", line 547, in from_indexpath_or_filestream\n",
      "    index_mtime = os.path.getmtime(indexpath)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"<frozen genericpath>\", line 55, in getmtime\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/media/lab/My Passport/hail/gfs/20200303/gfs.0p25.2019031500.f015.grib2.47d85.idx'\n",
      "Can't create file '/media/lab/My Passport/hail/gfs/20200303/gfs.0p25.2019031500.f018.grib2.47d85.idx'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lab/anaconda3/lib/python3.11/site-packages/cfgrib/messages.py\", line 538, in from_indexpath_or_filestream\n",
      "    self = cls.from_fieldset(filestream, index_keys, computed_keys)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lab/anaconda3/lib/python3.11/site-packages/cfgrib/messages.py\", line 378, in from_fieldset\n",
      "    return cls.from_fieldset_and_iteritems(fieldset, iteritems, index_keys, computed_keys)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lab/anaconda3/lib/python3.11/site-packages/cfgrib/messages.py\", line 391, in from_fieldset_and_iteritems\n",
      "    for field_id, raw_field in iteritems:\n",
      "  File \"/home/lab/anaconda3/lib/python3.11/site-packages/cfgrib/messages.py\", line 291, in __iter__\n",
      "    for message in self.itervalues():\n",
      "  File \"/home/lab/anaconda3/lib/python3.11/site-packages/cfgrib/messages.py\", line 267, in itervalues\n",
      "    with open(self.filestream.path, \"rb\") as file:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/media/lab/My Passport/hail/gfs/20200303/gfs.0p25.2019031500.f018.grib2'\n",
      "Can't read index file '/media/lab/My Passport/hail/gfs/20200303/gfs.0p25.2019031500.f018.grib2.47d85.idx'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lab/anaconda3/lib/python3.11/site-packages/cfgrib/messages.py\", line 547, in from_indexpath_or_filestream\n",
      "    index_mtime = os.path.getmtime(indexpath)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"<frozen genericpath>\", line 55, in getmtime\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/media/lab/My Passport/hail/gfs/20200303/gfs.0p25.2019031500.f018.grib2.47d85.idx'\n",
      "Can't create file '/media/lab/My Passport/hail/gfs/20200303/gfs.0p25.2019031500.f021.grib2.47d85.idx'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lab/anaconda3/lib/python3.11/site-packages/cfgrib/messages.py\", line 538, in from_indexpath_or_filestream\n",
      "    self = cls.from_fieldset(filestream, index_keys, computed_keys)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lab/anaconda3/lib/python3.11/site-packages/cfgrib/messages.py\", line 378, in from_fieldset\n",
      "    return cls.from_fieldset_and_iteritems(fieldset, iteritems, index_keys, computed_keys)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lab/anaconda3/lib/python3.11/site-packages/cfgrib/messages.py\", line 391, in from_fieldset_and_iteritems\n",
      "    for field_id, raw_field in iteritems:\n",
      "  File \"/home/lab/anaconda3/lib/python3.11/site-packages/cfgrib/messages.py\", line 291, in __iter__\n",
      "    for message in self.itervalues():\n",
      "  File \"/home/lab/anaconda3/lib/python3.11/site-packages/cfgrib/messages.py\", line 267, in itervalues\n",
      "    with open(self.filestream.path, \"rb\") as file:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/media/lab/My Passport/hail/gfs/20200303/gfs.0p25.2019031500.f021.grib2'\n",
      "Can't read index file '/media/lab/My Passport/hail/gfs/20200303/gfs.0p25.2019031500.f021.grib2.47d85.idx'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lab/anaconda3/lib/python3.11/site-packages/cfgrib/messages.py\", line 547, in from_indexpath_or_filestream\n",
      "    index_mtime = os.path.getmtime(indexpath)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"<frozen genericpath>\", line 55, in getmtime\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/media/lab/My Passport/hail/gfs/20200303/gfs.0p25.2019031500.f021.grib2.47d85.idx'\n",
      "Can't create file '/media/lab/My Passport/hail/gfs/20200303/gfs.0p25.2019031500.f024.grib2.47d85.idx'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lab/anaconda3/lib/python3.11/site-packages/cfgrib/messages.py\", line 538, in from_indexpath_or_filestream\n",
      "    self = cls.from_fieldset(filestream, index_keys, computed_keys)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lab/anaconda3/lib/python3.11/site-packages/cfgrib/messages.py\", line 378, in from_fieldset\n",
      "    return cls.from_fieldset_and_iteritems(fieldset, iteritems, index_keys, computed_keys)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lab/anaconda3/lib/python3.11/site-packages/cfgrib/messages.py\", line 391, in from_fieldset_and_iteritems\n",
      "    for field_id, raw_field in iteritems:\n",
      "  File \"/home/lab/anaconda3/lib/python3.11/site-packages/cfgrib/messages.py\", line 291, in __iter__\n",
      "    for message in self.itervalues():\n",
      "  File \"/home/lab/anaconda3/lib/python3.11/site-packages/cfgrib/messages.py\", line 267, in itervalues\n",
      "    with open(self.filestream.path, \"rb\") as file:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/media/lab/My Passport/hail/gfs/20200303/gfs.0p25.2019031500.f024.grib2'\n",
      "Can't read index file '/media/lab/My Passport/hail/gfs/20200303/gfs.0p25.2019031500.f024.grib2.47d85.idx'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lab/anaconda3/lib/python3.11/site-packages/cfgrib/messages.py\", line 547, in from_indexpath_or_filestream\n",
      "    index_mtime = os.path.getmtime(indexpath)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"<frozen genericpath>\", line 55, in getmtime\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/media/lab/My Passport/hail/gfs/20200303/gfs.0p25.2019031500.f024.grib2.47d85.idx'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping file gfs.0p25.2019031500.f003.grib2 due to error: [Errno 2] No such file or directory: '/media/lab/My Passport/hail/gfs/20200303/gfs.0p25.2019031500.f003.grib2'\n",
      "Skipping file gfs.0p25.2019031500.f006.grib2 due to error: [Errno 2] No such file or directory: '/media/lab/My Passport/hail/gfs/20200303/gfs.0p25.2019031500.f006.grib2'\n",
      "Skipping file gfs.0p25.2019031500.f009.grib2 due to error: [Errno 2] No such file or directory: '/media/lab/My Passport/hail/gfs/20200303/gfs.0p25.2019031500.f009.grib2'\n",
      "Skipping file gfs.0p25.2019031500.f012.grib2 due to error: [Errno 2] No such file or directory: '/media/lab/My Passport/hail/gfs/20200303/gfs.0p25.2019031500.f012.grib2'\n",
      "Skipping file gfs.0p25.2019031500.f015.grib2 due to error: [Errno 2] No such file or directory: '/media/lab/My Passport/hail/gfs/20200303/gfs.0p25.2019031500.f015.grib2'\n",
      "Skipping file gfs.0p25.2019031500.f018.grib2 due to error: [Errno 2] No such file or directory: '/media/lab/My Passport/hail/gfs/20200303/gfs.0p25.2019031500.f018.grib2'\n",
      "Skipping file gfs.0p25.2019031500.f021.grib2 due to error: [Errno 2] No such file or directory: '/media/lab/My Passport/hail/gfs/20200303/gfs.0p25.2019031500.f021.grib2'\n",
      "Skipping file gfs.0p25.2019031500.f024.grib2 due to error: [Errno 2] No such file or directory: '/media/lab/My Passport/hail/gfs/20200303/gfs.0p25.2019031500.f024.grib2'\n",
      "No data extracted. Please check the input files.\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import os\n",
    "\n",
    "# Folder containing your GRIB files\n",
    "input_folder = '/media/lab/My Passport/hail/gfs/20200303/'\n",
    "# List of GRIB files to process\n",
    "grib_files = [\n",
    "    f\".f{str(i).zfill(3)}.grib2\" for i in range(0, 25, 3)\n",
    "]\n",
    "# Path for the output NetCDF file\n",
    "output_file = '/media/lab/My Passport/hail/gfs/20200303/temperature_heightAboveGround20200303_2m.nc'\n",
    "\n",
    "# Initialize an empty list to collect the datasets\n",
    "datasets = []\n",
    "\n",
    "# Loop through each file and extract Temperature data at heightAboveGround level of 2m (or nearest)\n",
    "for grib_file in grib_files:\n",
    "    file_path = os.path.join(input_folder, grib_file)\n",
    "    try:\n",
    "        # Open dataset for Temperature at heightAboveGround levels\n",
    "        ds = xr.open_dataset(\n",
    "            file_path,\n",
    "            engine='cfgrib',\n",
    "            backend_kwargs={'filter_by_keys': {'typeOfLevel': 'heightAboveGround', 'shortName': 't'}}\n",
    "        )\n",
    "        \n",
    "        # Select only the Temperature variable at the 2-meter level, allowing the nearest level if 2m is missing\n",
    "        ds_filtered = ds.sel(heightAboveGround=2, method='nearest')\n",
    "        \n",
    "        # Append dataset if it contains the expected data\n",
    "        if 't' in ds_filtered.data_vars:\n",
    "            datasets.append(ds_filtered)\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping file {grib_file} due to error: {e}\")\n",
    "\n",
    "# Concatenate all datasets along the time dimension if any datasets were successfully loaded\n",
    "if datasets:\n",
    "    # Concatenate all datasets along the time dimension, preserving heightAboveGround levels\n",
    "    combined_ds = xr.concat(datasets, dim='time', coords='minimal', compat='override')\n",
    "    \n",
    "    # Save to NetCDF file\n",
    "    combined_ds.to_netcdf(output_file)\n",
    "    print(f\"Data successfully saved to {output_file}\")\n",
    "else:\n",
    "    print(\"No data extracted. Please check the input files.\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8937fc45-f1f1-4a73-ab5a-53c003079504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping file gfs.0p25.2019031500.f000.grib2 due to error: \"'heightAboveGround' is not a valid dimension or coordinate\"\n",
      "Skipping file gfs.0p25.2019031500.f003.grib2 due to error: \"'heightAboveGround' is not a valid dimension or coordinate\"\n",
      "Skipping file gfs.0p25.2019031500.f006.grib2 due to error: \"'heightAboveGround' is not a valid dimension or coordinate\"\n",
      "Skipping file gfs.0p25.2019031500.f009.grib2 due to error: \"'heightAboveGround' is not a valid dimension or coordinate\"\n",
      "Skipping file gfs.0p25.2019031500.f012.grib2 due to error: \"'heightAboveGround' is not a valid dimension or coordinate\"\n",
      "Skipping file gfs.0p25.2019031500.f015.grib2 due to error: \"'heightAboveGround' is not a valid dimension or coordinate\"\n",
      "Skipping file gfs.0p25.2019031500.f018.grib2 due to error: \"'heightAboveGround' is not a valid dimension or coordinate\"\n",
      "Skipping file gfs.0p25.2019031500.f021.grib2 due to error: \"'heightAboveGround' is not a valid dimension or coordinate\"\n",
      "Skipping file gfs.0p25.2019031500.f024.grib2 due to error: \"'heightAboveGround' is not a valid dimension or coordinate\"\n",
      "No data extracted. Please check the input files.\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import os\n",
    "\n",
    "# Folder containing your GRIB files\n",
    "input_folder = '/media/lab/My Passport/hail/gfs/20190315/'\n",
    "# List of GRIB files to process\n",
    "grib_files = [\n",
    "    f\"gfs.0p25.2019031500.f{str(i).zfill(3)}.grib2\" for i in range(0, 25, 3)\n",
    "]\n",
    "# Path for the output NetCDF file\n",
    "output_file = '/media/lab/My Passport/hail/gfs/20190315/relative_humidity_heightAboveGround_2m.nc'\n",
    "\n",
    "# Initialize an empty list to collect the datasets\n",
    "datasets = []\n",
    "\n",
    "# Loop through each file and extract Relative Humidity data at heightAboveGround level of 2m (or nearest)\n",
    "for grib_file in grib_files:\n",
    "    file_path = os.path.join(input_folder, grib_file)\n",
    "    try:\n",
    "        # Open dataset for Relative Humidity at heightAboveGround levels\n",
    "        ds = xr.open_dataset(\n",
    "            file_path,\n",
    "            engine='cfgrib',\n",
    "            backend_kwargs={'filter_by_keys': {'typeOfLevel': 'heightAboveGround', 'shortName': 'r'}}\n",
    "        )\n",
    "        \n",
    "        # Select only the Relative Humidity variable at the 2-meter level, allowing the nearest level if 2m is missing\n",
    "        ds_filtered = ds.sel(heightAboveGround=2, method='nearest')\n",
    "        \n",
    "        # Append dataset if it contains the expected data\n",
    "        if 'r' in ds_filtered.data_vars:\n",
    "            datasets.append(ds_filtered)\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping file {grib_file} due to error: {e}\")\n",
    "\n",
    "# Concatenate all datasets along the time dimension if any datasets were successfully loaded\n",
    "if datasets:\n",
    "    # Concatenate all datasets along the time dimension, preserving heightAboveGround levels\n",
    "    combined_ds = xr.concat(datasets, dim='time', coords='minimal', compat='override')\n",
    "    \n",
    "    # Save to NetCDF file\n",
    "    combined_ds.to_netcdf(output_file)\n",
    "    print(f\"Data successfully saved to {output_file}\")\n",
    "else:\n",
    "    print(\"No data extracted. Please check the input files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b837f4be-888c-4562-b453-4d1629cffc56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
